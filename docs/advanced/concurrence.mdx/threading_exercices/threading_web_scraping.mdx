---
sidebar_label: Web scraping
sidebar_position: 4
description: Exercice guidÃ© de web scraping concurrent avec threads
---

# ğŸŒ Web scraping concurrent

## ğŸ¯ Objectif pÃ©dagogique

- Comprendre pourquoi le threading est utile pour les I/O rÃ©seau.
- Ã‰crire un tÃ©lÃ©chargeur de pages web sÃ©quentiel puis concurrent.
- Mesurer et comparer les performances.

## ğŸ”° DÃ©marrage rapide â€” TÃ©lÃ©charger une page avec httpbin

Avant d'aborder le threading, voici un mini-exemple trÃ¨s simple qui tÃ©lÃ©charge
une page de test depuis https://httpbin.org. Ce service est conÃ§u pour
expÃ©rimenter les requÃªtes HTTP et renvoyer ce que vous lui envoyez.

Exemple minimal:

```python
import time
import urllib.request
import urllib.parse
import urllib.error

def telecharger_page(url, params=None, headers=None, timeout=10):
   """TÃ©lÃ©charge une page et retourne des mÃ©triques de base.

   Args:
      url (str): URL Ã  tÃ©lÃ©charger
      params (dict|None): paramÃ¨tres de requÃªte (?a=b)
      headers (dict|None): en-tÃªtes HTTP
      timeout (int|float): dÃ©lai max en secondes

   Returns:
      dict: informations sur la rÃ©ponse ou l'erreur
   """
   debut = time.time()
   try:
      # Construire l'URL avec les paramÃ¨tres
      if params:
         query_string = urllib.parse.urlencode(params)
         url = f"{url}?{query_string}"
      
      # CrÃ©er la requÃªte avec User-Agent par dÃ©faut
      req = urllib.request.Request(url)
      
      # Ajouter un User-Agent par dÃ©faut pour Ã©viter les blocages
      req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
      
      # Ajouter les en-tÃªtes personnalisÃ©s
      if headers:
         for key, value in headers.items():
            req.add_header(key, value)
      
      # Effectuer la requÃªte
      with urllib.request.urlopen(req, timeout=timeout) as resp:
         content = resp.read()
         duree = time.time() - debut
         return {
            "url": url,
            "url_finale": resp.url,
            "status": resp.status,
            "taille": len(content),
            "duree": duree,
            "content_type": resp.headers.get("Content-Type", ""),
            "succes": True,
         }
   except urllib.error.HTTPError as e:
      duree = time.time() - debut
      return {"url": url, "erreur": "HTTPError", "details": f"Code {e.code}: {e.reason}",
              "duree": duree, "succes": False}
   except urllib.error.URLError as e:
      duree = time.time() - debut
      return {"url": url, "erreur": "URLError", "details": str(e.reason),
              "duree": duree, "succes": False}
   except Exception as e:
      duree = time.time() - debut
      return {"url": url, "erreur": "Exception", "details": str(e), "duree": duree, "succes": False}

# 1) Appel simple: endpoint /get avec paramÃ¨tres (renvoie les params envoyÃ©s)
params = {"q": "python threading", "lang": "fr", "page": 1}
resultat = telecharger_page("https://httpbin.org/get", params=params)
print(resultat)

# 2) Endpoint qui simule un dÃ©lai rÃ©seau
resultat_delay = telecharger_page("https://httpbin.org/delay/1")
print(resultat_delay)

# 3) Endpoint qui renvoie du HTML simple
html_res = telecharger_page("https://httpbin.org/html")
print(html_res)
```

Adresses utiles pour vos tests:
- https://httpbin.org/get â€” renvoie les query params envoyÃ©s
- https://httpbin.org/delay/1 â€” simule 1 seconde d'attente (essayez 2, 3â€¦)
- https://httpbin.org/status/200 â€” retourne un code HTTP prÃ©cis (200, 404, 500â€¦)
- https://httpbin.org/json â€” renvoie un JSON
- https://httpbin.org/html â€” renvoie une page HTML


## ğŸ“œ Ã‰noncÃ©

Vous allez construire pas Ã  pas un petit scraper web, d'abord sÃ©quentiel puis
avec threads.

### Partie 1 â€” TÃ©lÃ©chargement sÃ©quentiel (baseline)

1. CrÃ©ez un fichier `scraping_baseline.py`.
3. Ã‰crivez `scraping_sequentiel(urls)` qui:
   - Boucle sur les URLs et appelle `telecharger_page`.
   - Stocke les rÃ©sultats dans une liste et mesure la durÃ©e totale.
   - Affiche le nombre de succÃ¨s/Ã©checs et le temps total.
4. Testez avec une boucle sur plusieurs URLs de `httpbin.org`.
5. Mesurez le temps total.

Attendu: fonctionnement correct mais temps total â‰ˆ somme des dÃ©lais.

### Partie 2 â€” Version avec threads

1. CrÃ©ez une fonction `scraping_avec_threads(urls)` qui:
   - Utilise `threading.Thread` pour lancer `telecharger_page` en parallÃ¨le, un thread par URL.
   - Partage une structure `resultats` (liste ou dict).
   - Mesure et affiche le temps total.
2. Rejouez le mÃªme jeu d'URLs et comparez les temps avec la baseline.

Attendu : fonctionnement correct et temps total â‰ª somme des dÃ©lais.

:::note

Il est souvent utile de limiter le nombre de threads actifs pour ne pas
surcharger le systÃ¨me ou le serveur distant. Par exemple, si vous avez 100 URLs Ã 
tÃ©lÃ©charger, vous ne voulez peut-Ãªtre pas lancer 100 threads en mÃªme temps. Dans 
la partie 2, nous laissons cette optimisation de cÃ´tÃ© pour la simplicitÃ©.

:::

### Partie 3 â€” Avec ThreadPoolExecutor

1. CrÃ©ez une fonction `scraping_avec_threadpool(urls, max_workers=5)` qui:
   - Utilise `concurrent.futures.ThreadPoolExecutor` pour gÃ©rer un pool de threads.
   - Soumet les tÃ¢ches de tÃ©lÃ©chargement et collecte les rÃ©sultats.
   - Mesure et affiche le temps total.
2. Testez et comparez les performances.