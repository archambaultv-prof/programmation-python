---
sidebar_label: ThreadPoolExecutor
sidebar_position: 8
description: ThreadPoolExecutor pour la gestion simplifi√©e des threads
---

# ThreadPoolExecutor pour g√©rer les threads

## Introduction

`ThreadPoolExecutor` est une abstraction de haut niveau qui simplifie grandement
la gestion des threads. Au lieu de cr√©er et g√©rer manuellement chaque thread,
vous d√©finissez un "pool" (bassin) de threads r√©utilisables qui ex√©cutent vos
t√¢ches. Cette approche est plus efficace, plus robuste et plus facile √† utiliser
que la gestion manuelle des threads.

### Avantages du ThreadPoolExecutor

- **R√©utilisation** : Les threads sont cr√©√©s une fois et r√©utilis√©s pour plusieurs t√¢ches
- **Gestion automatique** : Plus besoin de g√©rer manuellement `start()` et `join()`
- **Interface future** : R√©cup√©ration facile des r√©sultats avec les objets Future
- **Contr√¥le de la charge** : Limitation automatique du nombre de threads simultan√©s
- **Exception handling** : Gestion simplifi√©e des erreurs dans les threads

### Comparaison avec la gestion manuelle

Avant d'explorer `ThreadPoolExecutor`, voyons la diff√©rence avec la gestion
manuelle de threads :

```python
import threading
import time
import concurrent.futures

def tache_simple(nom, duree):
    """T√¢che qui simule du travail et retourne un r√©sultat"""
    print(f"D√©but de {nom}")
    time.sleep(duree)
    resultat = f"R√©sultat de {nom}"
    print(f"Fin de {nom}")
    return resultat

# ‚ùå M√©thode manuelle (complexe et verbeuse)
print("=== Gestion manuelle des threads ===")
threads = []
resultats = []

def wrapper(nom, duree, resultats_list):
    """Wrapper n√©cessaire pour capturer les r√©sultats"""
    resultat = tache_simple(nom, duree)
    resultats_list.append(resultat)

# Cr√©er et d√©marrer manuellement
for i in range(3):
    t = threading.Thread(target=wrapper, args=(f"T√¢che-{i+1}", 1, resultats))
    threads.append(t)
    t.start()

# Attendre manuellement
for t in threads:
    t.join()

print(f"R√©sultats manuels: {resultats}")

# ‚úÖ M√©thode ThreadPoolExecutor (simple et √©l√©gante)
print("\n=== Avec ThreadPoolExecutor ===")
with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    # Soumettre toutes les t√¢ches
    futures = [executor.submit(tache_simple, f"T√¢che-{i+1}", 1) for i in range(3)]
    
    # R√©cup√©rer les r√©sultats
    resultats_pool = [future.result() for future in futures]

print(f"R√©sultats pool: {resultats_pool}")
```

## M√©thodes principales

Le `ThreadPoolExecutor` offre plusieurs m√©thodes pour soumettre et g√©rer les t√¢ches.

### Submit() - Soumettre une t√¢che

La m√©thode `submit()` soumet une t√¢che au pool et retourne imm√©diatement un
objet `Future` qui repr√©sente l'ex√©cution en cours.

```python
import concurrent.futures
import time
import random

def calculer_carre(nombre):
    """Calcule le carr√© d'un nombre avec un d√©lai simul√©"""
    duree = random.uniform(0.5, 2.0)  # D√©lai al√©atoire pour simuler des t√¢ches variables
    print(f"Calcul de {nombre}¬≤ commenc√© (dur√©e: {duree:.1f}s)")
    time.sleep(duree)
    resultat = nombre ** 2
    print(f"Calcul de {nombre}¬≤ termin√© = {resultat}")
    return resultat

# Utilisation de submit()
print("=== Utilisation de submit() ===")
with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
    # Soumettre plusieurs t√¢ches et r√©cup√©rer les Future
    future1 = executor.submit(calculer_carre, 3)
    future2 = executor.submit(calculer_carre, 5)
    future3 = executor.submit(calculer_carre, 7)
    
    print("Toutes les t√¢ches soumises, en attente des r√©sultats...")
    
    # R√©cup√©rer les r√©sultats (bloque jusqu'√† ce que la t√¢che soit termin√©e)
    print(f"3¬≤ = {future1.result()}")  # Bloque jusqu'au r√©sultat
    print(f"5¬≤ = {future2.result()}")  # Bloque jusqu'au r√©sultat  
    print(f"7¬≤ = {future3.result()}")  # Bloque jusqu'au r√©sultat

print("Tous les calculs termin√©s")
```

### Map() - Appliquer une fonction √† une s√©quence

La m√©thode `map()` est similaire √† la fonction `map()` int√©gr√©e de Python, mais
ex√©cute la fonction en parall√®le sur tous les √©l√©ments.

```python
import concurrent.futures
import time

def traiter_element(element):
    """Traite un √©l√©ment avec simulation d'une op√©ration co√ªteuse
    
    Cela pourrait repr√©senter :
    - T√©l√©chargement d'une URL
    - Traitement d'un fichier
    - Requ√™te vers une base de donn√©es
    """
    print(f"Traitement de {element}")
    time.sleep(1)  # Simule une op√©ration qui prend du temps
    return element.upper()

# Donn√©es √† traiter
elements = ["pomme", "banane", "orange", "kiwi", "mangue"]

# ‚ùå Traitement s√©quentiel
print("=== Traitement s√©quentiel ===")
start = time.time()
resultats_sequentiel = [traiter_element(elem) for elem in elements]
duree_sequentiel = time.time() - start
print(f"R√©sultats: {resultats_sequentiel}")
print(f"Dur√©e: {duree_sequentiel:.2f}s")

# ‚úÖ Traitement parall√®le avec map()
print("\n=== Traitement parall√®le avec map() ===")
start = time.time()
with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    resultats_parallele = list(executor.map(traiter_element, elements))
duree_parallele = time.time() - start
print(f"R√©sultats: {resultats_parallele}")
print(f"Dur√©e: {duree_parallele:.2f}s")
print(f"Acc√©l√©ration: {duree_sequentiel/duree_parallele:.1f}x")
```

### As_completed() - Traiter les r√©sultats d√®s leur disponibilit√©

Parfois, vous voulez traiter les r√©sultats d√®s qu'ils sont disponibles, sans
attendre que toutes les t√¢ches se terminent. `as_completed()` est parfait pour
cela.

```python
import concurrent.futures
import time
import random

def tache_duree_variable(nom):
    """T√¢che avec une dur√©e d'ex√©cution variable"""
    duree = random.uniform(1, 4)  # Entre 1 et 4 secondes
    print(f"[{nom}] D√©marrage (dur√©e pr√©vue: {duree:.1f}s)")
    time.sleep(duree)
    print(f"[{nom}] Termin√©!")
    return f"R√©sultat de {nom}"

print("=== Traitement avec as_completed() ===")
print("Les r√©sultats s'affichent d√®s qu'ils sont pr√™ts:")

with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    # Soumettre plusieurs t√¢ches avec des dur√©es variables
    futures = {
        executor.submit(tache_duree_variable, f"T√¢che-{i+1}"): f"T√¢che-{i+1}" 
        for i in range(5)
    }
    
    # Traiter les r√©sultats dans l'ordre de completion (pas de soumission)
    for future in concurrent.futures.as_completed(futures):
        nom_tache = futures[future]
        try:
            resultat = future.result()
            print(f"‚úÖ {nom_tache} termin√©e: {resultat}")
        except Exception as e:
            print(f"‚ùå {nom_tache} a √©chou√©: {e}")

print("Toutes les t√¢ches trait√©es")
```

## Gestion des erreurs et exceptions

Une grande force de `ThreadPoolExecutor` est sa gestion simplifi√©e des
exceptions qui peuvent survenir dans les threads. Contrairement aux threads
classiques o√π les exceptions sont "perdues", le `ThreadPoolExecutor` capture
automatiquement toutes les exceptions et les rend disponibles via l'objet
`Future`.

### M√©canisme de propagation des exceptions

Le point cl√© √† comprendre est que **c'est l'appel √† `future.result()` qui l√®ve
l'exception**, pas le thread lui-m√™me. Voici ce qui se passe :

1. **Dans le thread** : L'exception est lev√©e et captur√©e automatiquement
2. **Stockage** : L'exception est stock√©e dans l'objet `Future`
3. **Propagation** : Quand `result()` est appel√©, l'exception est re-lev√©e dans le thread principal
4. **Gestion** : Vous pouvez alors la capturer avec un `try/except` classique

```python
import concurrent.futures
import time
import random

def tache_potentiellement_faillible(numero):
    """T√¢che qui peut √©chouer de mani√®re al√©atoire"""
    print(f"[T√¢che {numero}] D√©marrage dans le thread")
    time.sleep(0.5)  # Simule du travail
    
    if random.random() < 0.3:  # 30% de chance d'√©chouer
        print(f"[T√¢che {numero}] ERREUR dans le thread!")
        raise ValueError(f"Erreur simul√©e dans la t√¢che {numero}")
    
    print(f"[T√¢che {numero}] Succ√®s dans le thread")
    return f"Succ√®s t√¢che {numero}"

print("=== M√©canisme de gestion des erreurs ===")

with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
    # Soumettre plusieurs t√¢ches
    futures = [executor.submit(tache_potentiellement_faillible, i) for i in range(6)]
    
    print("Toutes les t√¢ches soumises, r√©cup√©ration des r√©sultats...")
    
    # Traiter les r√©sultats avec gestion d'erreur
    for i, future in enumerate(futures):
        try:
            # ‚ö†Ô∏è C'EST ICI QUE L'EXCEPTION EST LEV√âE (pas dans le thread)
            # Si une exception s'est produite dans le thread, result() la re-l√®ve
            resultat = future.result(timeout=5)  # Timeout de 5 secondes
            print(f"‚úÖ T√¢che {i}: {resultat}")
            
        except ValueError as e:
            # Exception sp√©cifique lev√©e par notre fonction
            print(f"‚ùå T√¢che {i} a √©chou√©: {e}")
            
        except concurrent.futures.TimeoutError:
            # Exception lev√©e si result() d√©passe le timeout
            print(f"‚è±Ô∏è T√¢che {i} timeout")
            
        except Exception as e:
            # Toute autre exception inattendue
            print(f"üí• T√¢che {i} erreur inattendue: {e}")

print("Traitement termin√© malgr√© les erreurs")
```

### V√©rification de l'√©tat sans lever d'exception

Vous pouvez √©galement v√©rifier l'√©tat d'un `Future` sans risquer de lever une
exception :

```python
import concurrent.futures
import time

def tache_avec_erreur():
    time.sleep(1)
    raise RuntimeError("Quelque chose a mal tourn√©")

def tache_normale():
    time.sleep(1)
    return "Tout va bien"

print("=== V√©rification d'√©tat des Future ===")

with concurrent.futures.ThreadPoolExecutor() as executor:
    future_erreur = executor.submit(tache_avec_erreur)
    future_normal = executor.submit(tache_normale)
    
    # Attendre que les t√¢ches se terminent
    time.sleep(2)
    
    # V√©rifier l'√©tat sans lever d'exception
    print(f"Future normal termin√©: {future_normal.done()}")
    print(f"Future erreur termin√©: {future_erreur.done()}")
    
    # Pour le future normal
    if future_normal.done():
        try:
            resultat = future_normal.result()
            print(f"‚úÖ R√©sultat normal: {resultat}")
        except Exception as e:
            print(f"‚ùå Erreur inattendue: {e}")
    
    # Pour le future avec erreur
    if future_erreur.done():
        try:
            resultat = future_erreur.result()  # Cette ligne l√®vera l'exception
            print(f"‚úÖ R√©sultat erreur: {resultat}")
        except RuntimeError as e:
            print(f"‚ùå Erreur attendue captur√©e: {e}")
```

## Param√®tres de configuration

### max_workers - Contr√¥ler le nombre de threads

Le param√®tre `max_workers` d√©termine le nombre maximum de threads dans le pool.

**Recommandations pour `max_workers` :**

- **T√¢ches I/O bound** : `max_workers` peut √™tre plus √©lev√© (par exemple, 2-4x le nombre de CPU)
- **T√¢ches CPU bound** : `max_workers` devrait √©galer le nombre de CPU (mais mieux vaut utiliser `ProcessPoolExecutor`)

## Bonnes pratiques

### 1. Utiliser le context manager (with)

Toujours utiliser `with` pour s'assurer que le pool est correctement ferm√© :

```python
# ‚úÖ Recommand√©
with concurrent.futures.ThreadPoolExecutor() as executor:
    # Votre code ici
    pass
# Le pool est automatiquement ferm√© ici

# ‚ùå √Ä √©viter
executor = concurrent.futures.ThreadPoolExecutor()
# Code...
executor.shutdown()  # Facile d'oublier!
```

### 2. Choisir le bon type de pool

- **ThreadPoolExecutor** : Pour les t√¢ches I/O bound (r√©seau, fichiers, base de donn√©es)
- **ProcessPoolExecutor** : Pour les t√¢ches CPU bound (calculs intensifs)

### 3. G√©rer les timeouts

```python
with concurrent.futures.ThreadPoolExecutor() as executor:
    future = executor.submit(tache_longue)
    try:
        resultat = future.result(timeout=30)  # Timeout de 30 secondes
    except concurrent.futures.TimeoutError:
        print("T√¢che trop longue, annulation...")
        # Note: La t√¢che continue en arri√®re-plan mais le r√©sultat est ignor√©
```

### 4. Limiter la taille des donn√©es

Ne pas passer d'√©normes structures de donn√©es aux threads. Pr√©f√©rez passer des
identifiants ou des chemins et laisser chaque thread charger ses propres donn√©es.

Le `ThreadPoolExecutor` est un outil puissant qui simplifie consid√©rablement la
programmation concurrente en Python. Il g√®re automatiquement la cr√©ation, la
r√©utilisation et la destruction des threads, tout en offrant une interface
intuitive pour soumettre des t√¢ches et r√©cup√©rer leurs r√©sultats.

